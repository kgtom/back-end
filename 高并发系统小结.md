### 一、重新定义高并发
单机服务器qps达到10000,可以称为高并发系统了；db服务器一般1000-1500，到达2000几乎到达顶峰了。

### 二、高并发瓶颈
 cpu、内存过高，数据库连接池打满(磁盘io、网络io)

### 三、高并发设计

* 拆分服务，分布式集群，单机能力是有限的，集群的能力解决高并发的问题
* 缓存：前端、CDN、应用层、数据库
* 分库分表:详见场景分析
* 限流:常用算法(令牌桶、漏桶、计数器)；接入层(ngx请求、连接数)；应用层(接口)；分布式(redis+lua);限制总资源数(db连接池)
* 降级:手动降级(配置文件)；自动降级(根据系统负载、资源使用情况：超时、失败次数、故障、hystrix)；
* 回滚:事务、代码库、部署版本、数据版本、静态资源
* 设置合理超时时间和重试次数:接入层(nginx客户端、dns超时)；数据库客户端超时；业务api超时
* 隔离：进程、集群、机房、读写

### 四、场景分析：以订单库为例
* 1.订单orderID的生成
 参考snowflake，生成非数字递增的id,包括五部分： 分库信息2位+分表信息3位+时间戳毫秒+机器号1位预分配+自增序号(同一毫秒内)
* 2.分库分表：用户维度拆分，比如分64个库 100张表，共6400张表；dbNO=userID%64+1 tableNO=userID%100+1;如果单机数据库并发1500--2000,则64*1500=9.6w,每秒接近10w并发。
* 3.6400张表，如果每一张表数据500w,则100*64*5000000=320亿，如果每天100w订单的话，320亿大约10年够用。
* 4.如果后续db需要扩容128台，则两种方案：
  - dba 使用工具同步数据到新库
  - 使用分库丢失精度？todo

* 多维度查询：订单库根据用户维度userID拆分，如果在商户维度查询的话，则三种方案：(本质是拿空间换时间)
   - 冗余方案：则商户维度冗余一份订单数据，通过mq异步处理，缺点占空间
   - 数据异构1：数据量小，存放在redis或者MongoDB中
   - 数据异构2：异构索引表尽量降低全表扫描频率。比如：主订单表按orderID分库分表会更均匀将数据分库分表存储；订单索引表按userID分库分表。查询userID=1000000订单，则先从订单索引表查出orderID，再从主表查询订单信息，两次查询。

* 5.数据存储级别
 - 核心数据，不使用缓存，直接操作db
 - 非核心，使用redis
 - 配置信息，放在内存中
* 6.限流：接入层Nginx层流量过滤
  比如：api最大处理qps10w，如果超过10w流量进来，则先放在Nginx层队列中，设置Nginx [max_conns](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) 队列最大qps50w,超过50w直接返回系统繁忙。api层每10w处理即可。



### ps：阿里云rds db性能测试
[阿里云](https://yq.aliyun.com/articles/708050?spm=5176.10695662.1996646101.searchclickresult.3a2513b7M3wylv&aly_as=u4Q-KJrN)
