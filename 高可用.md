---


---

<h2 id="一、前言">一、前言</h2>
<p>系统刚搭建完成好比建造了一个<strong>土房子</strong>，考虑自己能在里面住就可以了，而发展到<strong>四合院</strong>，就要考虑东厢、西厢等一些布局的问题，再发展到一个<strong>小区</strong>，就涉及内部道路、配套等等，再大一点发展成一个<strong>城市</strong>，就要涉及要各种保障工作，比如下雨了怎么排水，进城要设置哪些关卡，哪些道路要放置监控等。</p>
<p>系统建设也是这样，一个用户基数小、日活少的系统，要考虑的主要是完成功能，随着业务量增大，就要开始涉及系统间的问题，接着去发展一些基础设施、共通组件等，而我们目前的局面是，系统已经扩大到了<strong>城市</strong>的规模，我们要解决更繁重的问题。</p>
<h2 id="二、发现问题">二、发现问题</h2>
<p>在很长时间里，我们做的只是在原有系统上增加功能，架构上没有大调整。但是随着业务增长，就算我们系统没有任何发版升级，也会突然出现一些事故。事故出现的频率越来越高，我们自身的升级，也经常是困难重重。基础设施升级、上下游升级，经常会发生“蝴蝶效应”，毫无征兆的受到影响。以下是这些现象的鱼骨图分析：</p>
<p><img src="http://39.106.173.209:88/content/images/2018/07/---.png" alt="---"></p>
<p>通过排查，我们发现了系统的主要问题，从大方面说就是：外部的问题和自身的问题，当然症结在于架构的问题。问题分类如下图所示：</p>
<p><img src="http://39.106.173.209:88//content/images/2018/07/--.png" alt="--"></p>
<h2 id="三、分析问题">三、分析问题</h2>
<h3 id="事务中包含外部调用">1. 事务中包含外部调用</h3>
<p>外部调用包括对外部系统的调用和基础组件的调用，如果外部调用超时，必然造成本次事务失败。</p>
<h3 id="超时时间和重试次数不合理">2.超时时间和重试次数不合理</h3>
<p>对外部系统和缓存、消息队列等基础组件的依赖，<br>
如果超时时间设置过长、重试过多，系统长时间不返回，可能会导致连接池被打满，系统死掉；<br>
如果超时时间设置过短，499错误会增多，系统的可用性会降低。<br>
如果超时时间设置得短，重试次数设置得多，会增加系统的整体耗时；<br>
如果超时时间设置得短，重试次数设置得也少，那么这次请求的返回结果会不准确</p>
<ul>
<li>外部依赖的地方没有熔断(降级)的问题(即：我们依赖的地方)<br>
系统没有熔断，如果由于代码逻辑问题上线引起故障、网络问题、调用超时、业务促销调用量激增、服务容量不足等原因，服务调用链路上有一个下游服务出现故障，就可能导致接入层其它的业务不可用。</li>
</ul>
<p><img src="http://39.106.173.209:88//content/images/2018/07/----1.png" alt="----1"></p>
<ul>
<li>
<p>依赖我们的地方没有限流的问题<br>
在开放式的网络环境下，对外系统往往会收到很多有意无意的恶意攻击，如DDoS攻击、用户失败重刷。<br>
<img src="http://39.106.173.209:88//content/images/2018/07/----2.png" alt="----2"></p>
</li>
<li>
<p>依赖不合理的问题<br>
每多一个依赖方，风险就会累加。特别是强依赖，它本身意味着一荣俱荣、一损俱损。</p>
</li>
<li>
<p>没有有效的资源隔离<br>
容易造成级联多米诺骨牌效应。</p>
</li>
<li>
<p>慢查询问题</p>
</li>
</ul>
<p><img src="http://39.106.173.209:88//content/images/2018/07/----3.png" alt="----3"></p>
<ul>
<li>废弃逻辑和临时代码<br>
过期的代码会对正常逻辑有干扰，让代码不清晰。特别是对新加入的同事，他们对明白干什么用的代码可以处理。但是已经废弃的和临时的，因为不知道干什么用的，所以改起来更忐忑。如果知道是废弃的，其它功能改了这一块没改，也有可能这块不兼容，引发问题。</li>
</ul>
<h2 id="四、解决问题如何实现高可用">四、解决问题(如何实现高可用)</h2>
<h3 id="高可用指标">1.高可用指标</h3>
<p>衡量一个系统的可用性有两个指标：</p>
<ol>
<li>MTBF (Mean Time Between Failure) 即平均多长时间不出故障；</li>
<li>MTTR (Mean Time To Recovery) 即出故障后的平均恢复时间。</li>
</ol>
<p><img src="http://39.106.173.209:88//content/images/2018/07/availability.png" alt="availability"></p>
<p>通过这两个指标可以计算出可用性，也就是我们大家比较熟悉的“几个9”。</p>
<p><img src="http://39.106.173.209:88//content/images/2018/07/availability-stander.png" alt="availability-stander"></p>
<p><img src="http://39.106.173.209:88//content/images/2018/07/-------.png" alt="-------"></p>
<p>因此提升系统的可用性，就得从这两个指标入手，要么降低故障恢复的时间，要么延长不出故障的时间。</p>
<ul>
<li>要降低故障恢复的时间，首先得尽早的发现故障，然后才能解决故障，这些故障包括系统内和系统外的，这就需要依赖业务监控系统。即：日志收集+搜索+显示(ELK)</li>
<li>要延长不出故障的时间，有以下方法，例如：对依赖我们的限流，我们自身要降级、回滚，我们依赖的要隔离，对于外部调用要有超时及重试机制，具体如下：</li>
</ul>
<h3 id="方法">2.方法</h3>
<h4 id="事务中不包含外部调用">1. 事务中不包含外部调用</h4>
<ul>
<li>排查各个系统的代码，检查在事务中是否存在RPC调用、HTTP调用、消息队列操作、缓存、循环查询等耗时的操作，这个操作应该移到事务之外，理想的情况是事务内只处理数据库操作。</li>
<li>将大事务拆分小事务，降低db的资源被长时间事务锁占用而造成db瓶颈</li>
<li>数据库事务异步化，基于消息服务提供异步机制。</li>
</ul>
<h4 id="设置合理的超时时间和重试次数">2.设置合理的超时时间和重试次数</h4>
<ul>
<li>设置超时时间原则：调用方的超时时间&gt;依赖服务自己的超时时间</li>
<li>重试次数：一般3次，否则可以不重试</li>
</ul>
<h4 id="外部依赖要做熔断，即：降级、回滚（上游死，我们不死）">3.外部依赖要做熔断，即：降级、回滚（<strong>上游死，我们不死</strong>）</h4>
<p>在依赖的服务不可用时，服务调用方应该通过一些技术手段，向上提供有损服务，保证业务柔性可用。</p>
<p><strong>举例:</strong><br>
对非关键路径上的服务故障做了降级。例如账号的一个查询服务依赖Redis，当Redis抖动的时候服务的可用性也随之降低，我们通过自研kv中间件做Redis的备用存储，当检测到Redis已经非常不可用时就切到kv中间件上。</p>
<h4 id="依赖我们地方要做限流（不被下游弄死）">4.依赖我们地方要做限流（<strong>不被下游弄死</strong>）</h4>
<ul>
<li>通过对服务端的业务性能压测，可以分析出一个相对合理的最大QPS。</li>
<li>流量控制中用的比较多的三个算法是<a href="https://github.com/kgtom/daily-life/blob/master/important/%E5%AF%B9%E9%AB%98%E5%B9%B6%E5%8F%91%E9%99%90%E6%B5%81%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83.md">令牌桶、漏桶、计数器</a>。<br>
<strong>举例:</strong><br>
同一个账号 在一分钟内，不能超过20次请求。可以通过设置计数器</li>
</ul>
<h4 id="依赖不合理地方-（做到自己不死）">5.依赖不合理地方 （<strong>做到自己不死</strong>）</h4>
<p><strong>要是有依赖就去依赖，去不了，就弱依赖，如果要强依赖，那就熔断。</strong></p>
<p>a.别人死我们不死：无依赖，</p>
<p>熔断：别人死了，我们快速失败，安全提示，例如：第三方登录超时，提示使用其他登录方式<br>
别人活了，快速恢复：心跳检查 或者 事件监听<br>
需要设置合理超时和重试、蓄洪、限流、熔断、降级</p>
<p>b.自己不作死：<br>
不作：不当小白鼠，用成熟的技术；规范研发流程、做好测试和演练<br>
不死：单个节点挂了，用集群；单个机房挂了，用多机房；整个地区网断，用多机房；即：异地多活<br>
涉及到集群和跨区的要有策略：负载均衡、主从切换，优先策略</p>
<p>例如 用户中心—账号模块具体做法：<br>
考虑到账号读多写少的特性（，我们采用了一主多从的数据库部署方案，优先解决读多活的问题。<br>
Redis如果也用一主多从的模式可行吗？答案是不行，因为Redis主从同步机制会优先尝试增量同步，当增量同步不成功时，再去尝试全量同步，一旦专线发生抖动就会把主库拖垮，并进一步阻塞专线，形成“雪崩效应”。因此两地的Redis只能是双主模式，但是这种架构有一个问题，就是我们得自己去解决数据同步的问题，除了保证数据不丢，还要保证数据一致。<br>
如何保障同步呢？<br>
1.基于nsq 消息队列<br>
2.定时任务去scan两地的数据做对比统计，如果发现有不一致的还能及时修复掉。</p>
<p>总体上账号的异地多活遵循以下三个原则：<br>
1.北上任何一地故障，另一地都可提供完整服务。<br>
2.北上两地同时对外提供服务，确保服务随时可用。<br>
3.两地服务都遵循BASE原则，确保数据最终一致。</p>
<p>c.不被队友搞死：<br>
队友：部门内部依赖关系<br>
需要明确边界，做到队友死自己不死。</p>
<p><strong>总结3、4、5：研发规范、自身稳定、容错下游、防御上游。</strong></p>
<h4 id="资源隔离">6.资源隔离</h4>
<p>进程、线程隔离；集群、机房隔离；读写隔离、动静隔离<br>
<strong>举例</strong><br>
当业务依赖的下游服务出故障时不影响自身的核心功能或服务。例如：用户登录账号对上层业务提供的鉴权和查询服务即核心服务，这些服务的QPS非常高，业务方对它们的可用性要求也很高，别说是服务故障，就连任何一点抖动都是不能接受的。对此我们先从整体架构上把服务拆分，其次在服务内对下游依赖做资源隔离，都尽可能的缩小故障发生时的影响范围。</p>
<h4 id="慢查询的问题">7.慢查询的问题</h4>
<p><strong>解决方法</strong>：</p>
<ul>
<li>将查询分成实时查询、近实时查询和离线查询。实时查询可穿透数据库，其它的不走数据库，可以用Elasticsearch来实现一个查询中心，处理近实时查询和离线查询。</li>
<li>读写分离。写走主库，读走从库。</li>
<li>索引优化。索引过多会影响数据库写性能。索引不够查询会慢。DBA建议一个数据表的索引数不超过4个，超过4个说明表结构不合理，需要考虑重新设计。</li>
<li>不允许出现大表。MySQL数据库的一张数据表当数据量达到千万级，效率开始急剧下降。</li>
</ul>
<h4 id="废弃逻辑和临时代码">8.废弃逻辑和临时代码</h4>
<ul>
<li>梳理每个接口的调用情况，对于没有调用量的接口，确认不再使用后及时下线。</li>
<li>code review保证每段逻辑都明白其含义，弄清楚是否是历史逻辑或者临时逻辑</li>
</ul>
<h3 id="五、持续跟进，定期检查checklist">五、持续跟进，定期检查checklist</h3>

<table>
<thead>
<tr>
<th>大分类</th>
<th>具体项</th>
</tr>
</thead>
<tbody>
<tr>
<td>基础组件</td>
<td>redis、mysql、nsq、日志(elk)</td>
</tr>
<tr>
<td>依赖内外部系统</td>
<td>超时时间、超时次数、幂等性、挂掉后是否有熔断</td>
</tr>
<tr>
<td>被依赖的内外部系统</td>
<td>超时时间、超时次数、幂等性、是否限流</td>
</tr>
<tr>
<td>核心api</td>
<td>qps、cup、内存使用率</td>
</tr>
</tbody>
</table><blockquote>
<p>reference:</p>
</blockquote>
<ul>
<li><a href="https://tech.meituan.com/dp_account_high_avaliable_road.html">meituan-account</a></li>
<li><a href="https://tech.meituan.com/Trade-High-Availability-in-Action.html">meituan-pay</a></li>
<li><a href="http://itindex.net/detail/58097-%E6%88%98%E7%8B%BC-%E9%A1%B9%E7%9B%AE-%E7%BE%8E%E5%9B%A2">meituan-zhanlang</a></li>
</ul>

