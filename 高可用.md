
## 学习大纲
* [一、前言？](#1)
* [二、发现问题](#2)
* [三、分析问题](#3)
* [四、解决问题(如何实现高可用)](#4)
* [五、总结](#5)


## 一、前言


   系统刚搭建完成好比建造了一个**土房子**，考虑自己能在里面住就可以了，而发展到**四合院**，就要考虑东厢、西厢等一些布局的问题，再发展到一个**小区**，就涉及内部道路、配套等等，再大一点发展成一个**城市**，就要涉及要各种保障工作，比如下雨了怎么排水，进城要设置哪些关卡，哪些道路要放置监控等。


![---](https://github.com/kgtom/back-end/blob/master/pic/system-grow.png)


系统建设也是这样，一个用户基数小、日活少的系统，要考虑的主要是完成功能，随着业务量增大，就要开始涉及系统间的问题，接着去发展一些基础设施、共通组件等，而我们目前的局面是，系统已经扩大到了**城市**的规模，我们要解决更繁重的问题。


## 二、发现问题

   在很长时间里，我们做的只是在原有系统上增加功能，架构上没有大调整。但是随着业务增长，就算我们系统没有任何发版升级，也会突然出现一些事故。事故出现的频率越来越高，我们自身的升级，也经常是困难重重。基础设施升级、上下游升级，经常会发生“蝴蝶效应”，毫无征兆的受到影响。以下是这些现象的鱼骨图分析：
   
   
![---](https://github.com/kgtom/back-end/blob/master/pic/%E9%B1%BC%E9%AA%A8%E5%9B%BE.png)

  通过排查，我们发现了系统的主要问题，从大方面说就是：外部的问题和自身的问题，当然症结在于架构的问题。问题分类如下图所示：
  
![--](https://github.com/kgtom/back-end/blob/master/pic/%E9%97%AE%E9%A2%98.png)

## 三、分析问题
### 1. 事务中包含外部调用

外部调用包括对外部系统的调用和基础组件的调用，如果外部调用超时，必然造成本次事务失败。
     
### 2.超时时间和重试次数不合理 
对外部系统和缓存、消息队列等基础组件的依赖，
如果超时时间设置过长、重试过多，系统长时间不返回，可能会导致连接池被打满，系统死掉；
如果超时时间设置过短，499错误会增多，系统的可用性会降低。
如果超时时间设置得短，重试次数设置得多，会增加系统的整体耗时；
如果超时时间设置得短，重试次数设置得也少，那么这次请求的返回结果会不准确


### 3. 外部依赖的地方没有熔断(降级)的问题(即：我们依赖的地方)
系统没有熔断，如果由于代码逻辑问题上线引起故障、网络问题、调用超时、业务促销调用量激增、服务容量不足等原因，服务调用链路上有一个下游服务出现故障，就可能导致接入层其它的业务不可用。

![----1](https://github.com/kgtom/back-end/blob/master/pic/%E6%97%A0%E7%86%94%E6%96%AD.png)


### 4. 依赖我们的地方没有限流的问题
在开放式的网络环境下，对外系统往往会收到很多有意无意的恶意攻击，如DDoS攻击、用户失败重刷。
![----2](https://github.com/kgtom/back-end/blob/master/pic/%E6%97%A0%E9%99%90%E6%B5%81.png)

### 5.依赖不合理的问题
 每多一个依赖方，风险就会累加。特别是强依赖，它本身意味着一荣俱荣、一损俱损。

### 6. 没有有效的资源隔离
 容易造成级联多米诺骨牌效应。

### 7. 慢查询问题

![----3](https://github.com/kgtom/back-end/blob/master/pic/%E6%85%A2%E6%9F%A5%E8%AF%A2.png)

### 8. 废弃逻辑和临时代码
过期的代码会对正常逻辑有干扰，让代码不清晰。特别是对新加入的同事，他们对明白干什么用的代码可以处理。但是已经废弃的和临时的，因为不知道干什么用的，所以改起来更忐忑。如果知道是废弃的，其它功能改了这一块没改，也有可能这块不兼容，引发问题。

## <span id="4">四、解决问题(如何实现高可用)</span>

### 1.高可用指标

高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统出故障的时间

衡量一个系统的可用性有两个指标：
*  MTBF (Mean Time Between Failure) 即平均多长时间不出故障；
*  MTTR (Mean Time To Recovery) 即出故障后的平均恢复时间。

![availability](https://github.com/kgtom/back-end/blob/master/pic/availability.png)

通过这两个指标可以计算出可用性，也就是我们大家比较熟悉的“几个9”。

![availability-stander](https://github.com/kgtom/back-end/blob/master/pic/availability-stander.png)


![-------](https://github.com/kgtom/back-end/blob/master/pic/%E5%8F%AF%E7%94%A8%E6%80%A7%E8%AE%A1%E7%AE%97.png)

因此提升系统的可用性，就得从这两个指标入手，要么降低故障恢复的时间，要么延长不出故障的时间。
 
* 要降低故障恢复的时间，首先得尽早的发现故障，然后才能解决故障，这些故障包括系统内和系统外的，这就需要依赖业务监控系统。即：日志收集+搜索+显示(ELK)
* 要延长不出故障的时间，有以下方法，对依赖我们的 **限流**，我们自身要 **降级**、**回滚**，我们依赖的要 **隔离**，对于外部调用要有 **超时及重试机制**。

### 2.方法

#### 1. 事务中不包含外部调用

 * 排查各个系统的代码，检查在事务中是否存在RPC调用、HTTP调用、消息队列操作、缓存、循环查询等耗时的操作，这个操作应该移到事务之外，理想的情况是事务内只处理数据库操作。
 * 将大事务拆分小事务，降低db的资源被长时间事务锁占用而造成db瓶颈
 * 数据库事务异步化，基于消息服务提供异步机制。

#### 2.设置合理的超时时间和重试次数

* 设置超时时间原则：调用方的超时时间>依赖服务自己的超时时间
* 重试次数：一般3次，否则可以不重试

#### 3.外部依赖要做熔断，即：降级、回滚（**上游死，我们不死**）

在依赖的服务不可用时，服务调用方应该通过一些技术手段，向上提供有损服务，保证业务柔性可用。
 
 **举例:**
 对非关键路径上的服务故障做了降级。例如账号的一个查询服务依赖Redis，当Redis抖动的时候服务的可用性也随之降低，我们通过自研kv中间件做Redis的备用存储，当检测到Redis已经非常不可用时就切到kv中间件上。
#### 4.依赖我们地方要做限流（**不被下游弄死**）

* 通过对服务端的业务性能压测，可以分析出一个相对合理的最大QPS。
* 流量控制中用的比较多的三个算法是[令牌桶、漏桶、计数器](https://github.com/kgtom/daily-life/blob/master/important/%E5%AF%B9%E9%AB%98%E5%B9%B6%E5%8F%91%E9%99%90%E6%B5%81%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83.md)。
 **举例:**
 同一个账号 在一分钟内，不能超过20次请求。可以通过设置计数器

#### 5.依赖不合理地方 （**做到自己不死**）

**要是有依赖就去依赖，去不了，就弱依赖，如果要强依赖，那就熔断。**

a.别人死我们不死：无依赖，

熔断：别人死了，我们快速失败，安全提示，例如：第三方登录超时，提示使用其他登录方式
     别人活了，快速恢复：心跳检查 或者 事件监听
     需要设置合理超时和重试、蓄洪、限流、熔断、降级


b.自己不作死：
不作：不当小白鼠，用成熟的技术；规范研发流程、做好测试和演练
不死：单个节点挂了，用集群；单个机房挂了，用多机房；整个地区网断，用多机房；即：异地多活
涉及到集群和跨区的要有策略：负载均衡、主从切换，优先策略

例如 用户中心---账号模块具体做法：
考虑到账号读多写少的特性（，我们采用了一主多从的数据库部署方案，优先解决读多活的问题。
Redis如果也用一主多从的模式可行吗？答案是不行，因为Redis主从同步机制会优先尝试增量同步，当增量同步不成功时，再去尝试全量同步，一旦专线发生抖动就会把主库拖垮，并进一步阻塞专线，形成“雪崩效应”。因此两地的Redis只能是双主模式，但是这种架构有一个问题，就是我们得自己去解决数据同步的问题，除了保证数据不丢，还要保证数据一致。
如何保障同步呢？
1.基于nsq 消息队列
2.定时任务去scan两地的数据做对比统计，如果发现有不一致的还能及时修复掉。

总体上账号的异地多活遵循以下三个原则：
1.北上任何一地故障，另一地都可提供完整服务。
2.北上两地同时对外提供服务，确保服务随时可用。
3.两地服务都遵循BASE原则，确保数据最终一致。

c.不被队友搞死：
队友：部门内部依赖关系
需要明确边界，做到队友死自己不死。

**总结3、4、5：研发规范、自身稳定、容错下游、防御上游。**

#### 6.资源隔离
  进程、线程隔离；集群、机房隔离；读写隔离、动静隔离
  **举例**
 当业务依赖的下游服务出故障时不影响自身的核心功能或服务。例如：用户登录账号对上层业务提供的鉴权和查询服务即核心服务，这些服务的QPS非常高，业务方对它们的可用性要求也很高，别说是服务故障，就连任何一点抖动都是不能接受的。对此我们先从整体架构上把服务拆分，其次在服务内对下游依赖做资源隔离，都尽可能的缩小故障发生时的影响范围。
#### 7.慢查询的问题
 **解决方法**：

* 将查询分成实时查询、近实时查询和离线查询。实时查询可穿透数据库，其它的不走数据库，可以用Elasticsearch来实现一个查询中心，处理近实时查询和离线查询。
* 读写分离。写走主库，读走从库。
* 索引优化。索引过多会影响数据库写性能。索引不够查询会慢。DBA建议一个数据表的索引数不超过4个，超过4个说明表结构不合理，需要考虑重新设计。
* 不允许出现大表。MySQL数据库的一张数据表当数据量达到千万级，效率开始急剧下降。


#### 8.废弃逻辑和临时代码
*   梳理每个接口的调用情况，对于没有调用量的接口，确认不再使用后及时下线。
*   code review保证每段逻辑都明白其含义，弄清楚是否是历史逻辑或者临时逻辑


### <span id="5">五、总结</span>


#### 1.持续跟进，定期检查checklist

 大分类 | 具体项
---- | ---|
基础组件 | redis、mysql、nsq、日志(elk)
依赖内外部系统 |  超时时间、超时次数、幂等性、挂掉后是否有熔断
被依赖的内外部系统|超时时间、超时次数、幂等性、是否限流
核心api|qps、cup、内存使用率


#### 2.总结
在分布式框架各个层都有进行 **冗余+故障转移**,即：一个挂了，用2个，两个挂了用集群，集群挂了用异地灾备。具体如下：

* 反向代理层nginx:使用[keepalived存活探测](https://www.cnblogs.com/kevingrace/p/6138185.html)和virtual IP提供服务。
* web站点层：使用nginx.conf配置多个web站点，nginx可以检测后端存活性
* server服务层：使用rancher 随机选取下游可用服务
* 缓存：redis天然支持主从同步，redis官方也有sentinel哨兵机制，来做redis的存活性检测，发现主redis挂了，快速转移到另一个从redis.
* 数据库：主从同步，读写分离，使用db 连接池发现挂了，快速转移到另一个db.

重点说一下 **server 服务层** 数据异构 处理。

##### 1.为什么要数据异构？
  * 场景：订单、用户、供应商。两个维度查询订单信息。在大流量数据后，为了减轻单库单表的压力，采用订单表水平切分，将用户id和供应商id 冗余到订单库中，其它表同样进行冗余处理。
  *  order1 (order_id,user_id,supplier_id) //按照order_id来分表
  *  user1(user_id,supplier_id,order_id) //按照user_id来分表
  *  supplier1(supplier_id,user_id,order_id) //按照supplier_id来分表

#### 2.如何做到数据异构

**重点：**数据异构前提保证数据有效性，即做好冗余及确保冗余准确性。
* 查询以用户表为例：(水平切分四个用户表)
 ~~~
 user_id = 17,
 17%4 + 1 = 2,   
 tableName := 'users'.'2'
 Select * from users2 where id = 17;
~~~
* 如何做到冗余呢？
  - 1.同步：三次db操作,订单表存储成功后，再新增用户、供应商表
  - 2.异步(mq)：订单表存储成功后，分表发送两个消息到消息队列，用户、供应商获取到后，执行存储操作。(为了保证ms成功，将订单表与发送mq记录表放到一个事务中)
  - 3.异步(定时服务)：订单表存储成功后，定时服务区执行用户、供应商存储
* 如何保证冗余数据准确性
  - 1.分表记录各自表log,针对log 增量数据比对，避免全表扫描
  - 2.使用mq，基于发布-订阅模式，order存储成功后，发送消息后user,user成功后发送消息后supplier，如果在允许最大时间内，没有收到消息，视为本次冗余操作失败。
* 数据异构：聚合三方数据，以kv形式存储(如Redis)，提供给前端展示。


> reference:
* [meituan-account](https://tech.meituan.com/dp_account_high_avaliable_road.html)
* [meituan-pay](https://tech.meituan.com/Trade-High-Availability-in-Action.html)
* [meituan-zhanlang](http://itindex.net/detail/58097-%E6%88%98%E7%8B%BC-%E9%A1%B9%E7%9B%AE-%E7%BE%8E%E5%9B%A2)
* [cnblogs](https://www.cnblogs.com/shizhiyi/p/7750530.html)

